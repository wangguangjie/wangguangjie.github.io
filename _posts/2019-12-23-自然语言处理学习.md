---
layout: post
title: "自然语言处理学习"
---
>记录自然语言处理的学习，学习资料主要为何晗的自然语言处理入门书籍和工具[HanLP](https://github.com/wangguangjie/HanLP)

### 环境配置
##### 1.1 Java
- 新建maven项目
- pom.xml添加依赖：
```
   <dependency>
     <groupId>com.hankcs</groupId>
     <artificatId>hanlp</artificatId>
     <version>portable-1.7.5</version>
```
> 零配置，即可使用基本功能（除由字构词、依存句法分析外的全部功能）。如果用户有自定义的需求，可以参考方式二，使用hanlp.properties进行配置（Portable版同样支持hanlp.properties）

##### 2.2 Python
- pip3 install pyhanlp
> 安装完成pyhanlp之后，初次运行hanlp时会下载数据集和配置文件
- hanlp -v
> 运行此命令后会看到数据集、配置文件和jar包的位置，为了使得Java的maven也能够使用全部功能只需要把properties配置文件复制到项目的resources目录下即可

##### 2.3 下载github源码
- git clone git@github.com:wangguangjie/HanLP.git
> 为了更好和结合书本与源码学习，下载源码到本地进行学习，但是源码没有完整的数据集，需要和上面同样的方法通过properties配置数据集，使得项目能够找到数据data



### 词典分词
>中文分词是将一段文本分为一系列单词的过程，大致分为**基于词典规则**和**基于机器学习**两大派别。词典分词是最简单、最常用的分词算法，仅需要一部词典和一套查词典的规则即可。

##### 2.1 什么是词
- 在语言学上，词的定义是具备独立意义的最小单位。
- 在基于词典的中文分词中，词的定义是：词典中的字符串就是词。(基于此定义那词典之外的词就不是词了，这也不符合实际，其实这也就是词典分词的弱点。事实上，词典是无法完整记录无穷的词汇，语言时时刻刻在变化，任何词典都只是某个时间节点拍摄的一张快照)
- 词的性质-**齐夫定律**
> 单词的词频和其排名成反比(词频排名越小，也就是越靠前，词频越高)

##### 2.2 词典
- HanLP词典
> /anaconda3/lib/python3.7/site-packages/pyhanlp/static/CoreNatureDictionary.mini.txt
```
   希望   v    386   n   36
```
别是单词、词性、词频，可能有多个词性，对应多个词频

##### 2.3 切分算法
> 常用规则：正向最长匹配、逆向最长匹配和双向最长匹配

- 完全切分
- 正向最长匹配
- 逆向最长匹配
- 双向最长

##### 2.4 字典树

##### 2.5 HanLP的词典分词实现

##### 2.6 准确率评估



### 二元语法与中文分词

##### 3.1 语言模型
- 数据稀疏
- 计算代价大

##### 3.2 中文分词语料库

##### 3.3 训练
- 加载语料库
- 统计一元语法
- 统计二元语法

##### 3.4 预测
- 加载模型
- 构建词网
- 节点间的距离计算
- 词图上的维特比算法
- 与用户词典的集成

##### 3.5 评测




### 隐马尔科夫模型与序列标注

##### 4.1 序列标注问题
- 序列标注与中文分词
- 序列标注与词性标注
- 序列标注与命名实体识别

##### 4.2 隐玛尔可夫模型
- 从玛尔可夫假设到隐玛尔可夫模型
- 初始状态概率向量
- 状态转移矩阵
- 发射概率矩阵
- 隐玛尔可夫模型的三个基本用法

##### 4.3 隐玛尔可夫模型的样本生成


##### 4.4 隐玛尔可夫模型的训练


##### 4.5 隐玛尔可夫模型的预测

##### 4.6 隐玛尔可夫模型应用于中文分词

##### 4.7 二阶隐玛尔可夫模型


### 感知机分类与序列标注


### 条件随机场与序列标注

### 词性标注

### 命名实体识别


### 信息抽取


### 文本聚类

### 文本分类


### 依存句法分析

### 深度学习与自然语言处理

























 




